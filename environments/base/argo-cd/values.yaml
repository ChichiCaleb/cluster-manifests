controller:
  replicas: 1 # Additional replicas will cause sharding of managed clusters across number of replicas.
  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus
  env:
    - name: ARGOCD_K8S_CLIENT_QPS 
      value: '300'

repoServer:
  autoscaling:
    enabled: true
    minReplicas: 1
  resources: # Adjust based on your specific use case (required for HPA)
    requests:
      cpu: '100m'
      memory: '256Mi'
    limits:
      cpu: '200m'
      memory: '512Mi'
  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus

applicationSet:
  replicas: 1
  # replicaCount: 1 # The controller doesn't scale horizontally, is active-standby replicas
  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus

server:
  autoscaling:
    enabled: true
    minReplicas: 1
  resources: # Adjust based on your specific use case (required for HPA)
    requests:
      cpu: '100m'
      memory: '256Mi'
    limits:
      cpu: '200m'
      memory: '512Mi'
  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus
  service:
    type: LoadBalancer
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
      service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing

configs:
  

  repositories:
    # Required when using helm repository with oci formal like karpenter 
    aws-public-ecr:
      name: aws-public-ecr
      type: helm
      url: public.ecr.aws
      enableOCI: 'true'
  rbac:
    policy.default: role:readonly
    policy.csv: |
      p, role:devops, applications, *,*/*,allow
      p, role:devops, clusters, get, *, allow
      p, role:devops, repositories, get, *, allow
      p, role:devops, repositories, create, *, allow
      p, role:devops, repositories, update, *, allow
      p, role:devops, repositories, delete, *, allow
      p, role:devops, gpgkeys, get, *, allow

      p, role:developers, applications, get, *, allow
      
      g, adminuser, role:admin
      g, devopsuser, role:devops
      g, devuser, role:developers
  cm:
    create: true

    accounts.adminuser: apiKey,login
    accounts.devopsuser: apiKey,login
    accounts.devuser: apiKey,login


    application.resourceTrackingMethod: 'annotation' 
    
    resource.customizations: |
      "alustan.io/*":
      health.lua: |
        health_status = {
          status = "Progressing",
          message = "Provisioning ..."
        }

        if obj.status == nil then
          return health_status
        end

        if obj.status == "error" or obj.status == "failed" then
          health_status.status = "Degraded"
          health_status.message = "Resource is in a degraded state."
        elseif obj.status == "success" then
          health_status.status = "Healthy"
          health_status.message = "Resource is up-to-date."
        elseif obj.status == "progressing" then
          health_status.status = "Progressing"
          health_status.message = "Resource is provisioning."
        else
          health_status.status = "Unknown"
          health_status.message = "Unknown status."
        end

        return health_status
      

extraObjects:
  - |
    apiVersion: argoproj.io/v1alpha1
    kind: AppProject
    metadata:
      name: default
      namespace: {{ $.Release.Namespace | quote }}
      annotations:
        source: gitops
    spec:
      clusterResourceWhitelist:
      - group: '*'
        kind: '*'
      sourceRepos:
        - '*'
      destinations:
        - namespace: '*'
          name: '*'
          server: '*'
          
